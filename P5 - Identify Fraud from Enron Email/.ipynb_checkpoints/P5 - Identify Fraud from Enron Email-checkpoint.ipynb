{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# P5 - Identify Fraud from Enron Email\n",
    "\n",
    "## Motivation\n",
    "\n",
    "This work is part of the Udacity Data Analyst Nanoprogram. This is the final project for the Machine Learning course. The goal of this project is to use machine learning algorithms to build an efficient classifier that can spot PoI (Person of Interest) in the Enron Dataset provided by the Udacity team.\n",
    "\n",
    "## Dataset\n",
    "\n",
    "The dataset used in this project has been extracted by the Udacity team from the huge Enron dataset. The original Enron dataset contains data from about 150 users, mostly senior management of Enron, organized into folders. The corpus contains a total of about 500 000 messages. This data was originally made public, and posted to the web, by the Federal Energy Regulatory Commission during its investigation. \n",
    "\n",
    "https://www.cs.cmu.edu/~./enron/\n",
    "\n",
    "The Udacity team extracted some useful features related to email correspondence for 145 users and enhanced it with financial data for these 145 users (salary, stock options, etc.).\n",
    "\n",
    "## Summary\n",
    "\n",
    "The first part is dedicated to analyze the features, get insights about the dataset, and spot the main outliers. Out of this, a cleaner dataset is created including only the features that will be used by the machine learning algorithms. \n",
    "\n",
    "Different basic machine learning algorithms are then tested, with limited success. In order to limit overfitting, manual feature selection is implemented using a decision tree classifier to estimate the different feature importance. After testing, it appears that this manual feature selection has a limited impact on the performance. Hence, another approach is used in the rest of the project to reduce overfitting. \n",
    "\n",
    "Machine learning algorithms are very sensible to hyper parameters. These can for instance play a strong role on overfitting. In order to determine the best parameters for this Enron Dataset, a Grid Search Analysis is done on different Classifiers. The Grid Search Analysis is done using a f-score as scorer similar to the one calculated by the tester.py file.\n",
    "\n",
    "In order to take the best out of these Classifiers, feature preparation is integrated in a pipeline estimator. More specifically, features are first scaled with a MinMax Scaler, then their dimension is reduced with a Principal Component Analysis before supplying them to the Classification Algorithm. \n",
    "\n",
    "Each classifier performance is calculated with the test_classifier function from the tester.py file. This function conducts a 1000-folds cross validation with the given classifier and the given feature list. It prints out different performance metrics (accuracy, precision, recall, f-scores) that help us assess the Classifier performance.\n",
    "\n",
    "In the end, the best classifier that pops out is an optimized Logistic Regression Classifier using a L1 distance penalty. This is the algorithm that has been implemented in my poi_id.py file. \n",
    "\n",
    "## Limitations and potentials\n",
    "\n",
    "One of the major limitation of this work was the dataset size and the numurous missing data it contains. There were only 145 usable observations to build a classifier and most of them were missing values for one or more features. This increased a lot the risk of overfitting. \n",
    "\n",
    "In order to tackle this issue, the StratifiedShuffleSplit function from the sickit learn cross-validation package was used to implement 10-folds to 1000-folds cross validation when developing and testing the different algorithms. \n",
    "\n",
    "In order to improve further the classifier performance, it would be interesting to try to implement some specific ensemble classifiers combining for example logistic regression with SVC or decision trees. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import pickle\n",
    "from time import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tester import dump_classifier_and_data, test_classifier\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.cross_validation import train_test_split, StratifiedShuffleSplit\n",
    "from sklearn.decomposition import RandomizedPCA\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "sys.path.append(\"../tools/\")\n",
    "from feature_format import featureFormat, targetFeatureSplit\n",
    "\n",
    "def scorer_f(clf, X, y):\n",
    "    '''\n",
    "    This function is used to evaluate the f-score from the tester.py file of a given classifier \n",
    "    on a given feature & label dataset. It is used in the grid search algorythm to select the best fit.\n",
    "    The code is extracted from the tester.py file.\n",
    "    '''\n",
    "    true_negatives = 0\n",
    "    false_negatives = 0\n",
    "    true_positives = 0\n",
    "    false_positives = 0\n",
    "    clf.fit(X, y)\n",
    "    predictions = clf.predict(X)\n",
    "    for prediction, truth in zip(predictions, y):\n",
    "        if prediction == 0 and truth == 0:\n",
    "            true_negatives += 1\n",
    "        elif prediction == 0 and truth == 1:\n",
    "            false_negatives += 1\n",
    "        elif prediction == 1 and truth == 0:\n",
    "            false_positives += 1\n",
    "        elif prediction == 1 and truth == 1:\n",
    "            true_positives += 1\n",
    "    try:\n",
    "        precision = 1.0*true_positives/(true_positives+false_positives)\n",
    "        recall = 1.0*true_positives/(true_positives+false_negatives)\n",
    "        f1 = 2.0 * true_positives/(2*true_positives + false_positives+false_negatives)\n",
    "        f2 = (1+2.0*2.0) * precision*recall/(4*precision + recall)\n",
    "        return f2\n",
    "    except ZeroDivisionError:\n",
    "        # print \"Got a divide by zero when trying out:\", clf\n",
    "        # print \"Precision or recall may be undefined due to a lack of true positive predicitons.\"\n",
    "        return 0\n",
    "\n",
    "# Load the dictionary containing the dataset\n",
    "with open(\"final_project_dataset.pkl\", \"r\") as data_file:\n",
    "    data_dict = pickle.load(data_file)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    salary to_messages deferral_payments total_payments  \\\n",
      "ALLEN PHILLIP K     201955        2902           2869717        4484442   \n",
      "BADUM JAMES P          NaN         NaN            178980         182466   \n",
      "BANNANTINE JAMES M     477         566               NaN         916197   \n",
      "BAXTER JOHN C       267102         NaN           1295738        5634343   \n",
      "BAY FRANKLIN R      239671         NaN            260455         827696   \n",
      "\n",
      "                   exercised_stock_options    bonus restricted_stock  \\\n",
      "ALLEN PHILLIP K                    1729541  4175000           126027   \n",
      "BADUM JAMES P                       257817      NaN              NaN   \n",
      "BANNANTINE JAMES M                 4046157      NaN          1757552   \n",
      "BAXTER JOHN C                      6680544  1200000          3942714   \n",
      "BAY FRANKLIN R                         NaN   400000           145796   \n",
      "\n",
      "                   shared_receipt_with_poi restricted_stock_deferred  \\\n",
      "ALLEN PHILLIP K                       1407                   -126027   \n",
      "BADUM JAMES P                          NaN                       NaN   \n",
      "BANNANTINE JAMES M                     465                   -560222   \n",
      "BAXTER JOHN C                          NaN                       NaN   \n",
      "BAY FRANKLIN R                         NaN                    -82782   \n",
      "\n",
      "                   total_stock_value           ...           loan_advances  \\\n",
      "ALLEN PHILLIP K              1729541           ...                     NaN   \n",
      "BADUM JAMES P                 257817           ...                     NaN   \n",
      "BANNANTINE JAMES M           5243487           ...                     NaN   \n",
      "BAXTER JOHN C               10623258           ...                     NaN   \n",
      "BAY FRANKLIN R                 63014           ...                     NaN   \n",
      "\n",
      "                   from_messages    other from_this_person_to_poi    poi  \\\n",
      "ALLEN PHILLIP K             2195      152                      65  False   \n",
      "BADUM JAMES P                NaN      NaN                     NaN  False   \n",
      "BANNANTINE JAMES M            29   864523                       0  False   \n",
      "BAXTER JOHN C                NaN  2660303                     NaN  False   \n",
      "BAY FRANKLIN R               NaN       69                     NaN  False   \n",
      "\n",
      "                   director_fees deferred_income long_term_incentive  \\\n",
      "ALLEN PHILLIP K              NaN        -3081055              304805   \n",
      "BADUM JAMES P                NaN             NaN                 NaN   \n",
      "BANNANTINE JAMES M           NaN           -5104                 NaN   \n",
      "BAXTER JOHN C                NaN        -1386055             1586055   \n",
      "BAY FRANKLIN R               NaN         -201641                 NaN   \n",
      "\n",
      "                                 email_address from_poi_to_this_person  \n",
      "ALLEN PHILLIP K        phillip.allen@enron.com                      47  \n",
      "BADUM JAMES P                              NaN                     NaN  \n",
      "BANNANTINE JAMES M  james.bannantine@enron.com                      39  \n",
      "BAXTER JOHN C                              NaN                     NaN  \n",
      "BAY FRANKLIN R             frank.bay@enron.com                     NaN  \n",
      "\n",
      "[5 rows x 21 columns]\n",
      "\n",
      "Total number of observations (including potential outliers):  146\n",
      "\n",
      "Number of PoI: 18 (12.3%)\n",
      "\n",
      "Total number of features:  20\n",
      "\n",
      "Number of Nan values for salary: 51 (34.9%)\n",
      "Number of Nan values for to_messages: 60 (41.1%)\n",
      "Number of Nan values for deferral_payments: 107 (73.3%)\n",
      "Number of Nan values for total_payments: 21 (14.4%)\n",
      "Number of Nan values for exercised_stock_options: 44 (30.1%)\n",
      "Number of Nan values for bonus: 64 (43.8%)\n",
      "Number of Nan values for restricted_stock: 36 (24.7%)\n",
      "Number of Nan values for shared_receipt_with_poi: 60 (41.1%)\n",
      "Number of Nan values for restricted_stock_deferred: 128 (87.7%)\n",
      "Number of Nan values for total_stock_value: 20 (13.7%)\n",
      "Number of Nan values for expenses: 51 (34.9%)\n",
      "Number of Nan values for loan_advances: 142 (97.3%)\n",
      "Number of Nan values for from_messages: 60 (41.1%)\n",
      "Number of Nan values for other: 53 (36.3%)\n",
      "Number of Nan values for from_this_person_to_poi: 60 (41.1%)\n",
      "Number of Nan values for director_fees: 129 (88.4%)\n",
      "Number of Nan values for deferred_income: 97 (66.4%)\n",
      "Number of Nan values for long_term_incentive: 80 (54.8%)\n",
      "Number of Nan values for email_address: 35 (24.0%)\n",
      "Number of Nan values for from_poi_to_this_person: 60 (41.1%)\n",
      "\n",
      "Features selected to be used by the Classifiers: \n",
      "['poi', 'salary', 'total_payments', 'total_stock_value', 'to_messages', 'shared_receipt_with_poi', 'from_this_person_to_poi', 'from_poi_to_this_person', 'from_messages']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Programmes\\Anaconda\\envs\\py27\\lib\\site-packages\\pandas\\core\\ops.py:714: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  result = getattr(x, name)(y)\n"
     ]
    }
   ],
   "source": [
    "# Import dataset into a pandas dataframe to easily manipulate the data\n",
    "df = pd. DataFrame.from_dict(data_dict, orient=\"index\")  \n",
    "print df.head(5) \n",
    "print \n",
    "\n",
    "# Total number of points\n",
    "print \"Total number of observations (including potential outliers): \", len(df)\n",
    "print\n",
    "\n",
    "\n",
    "# Allocation accross classes\n",
    "print \"Number of PoI: {0} ({1:.1f}%)\".format(sum(df.poi), 100.0*sum(df.poi)/len(df.poi))\n",
    "print\n",
    "\n",
    "# Total nubmer of features\n",
    "print \"Total number of features: \", len(df.columns)-1\n",
    "print\n",
    "\n",
    "# Missing values\n",
    "for feat in df.columns:\n",
    "    try:\n",
    "        n_NaN = sum(df[feat]==\"NaN\")\n",
    "        print \"Number of Nan values for {0}: {1} ({2:.1f}%)\".format(feat, n_NaN, 100.0* n_NaN/len(df.poi) )\n",
    "    except TypeError:\n",
    "        pass\n",
    "print\n",
    "\n",
    "# features_list is a list of strings, each of which is a feature name.\n",
    "# The first feature must be \"poi\".\n",
    "features_list = ['poi', 'salary', 'total_payments', \"total_stock_value\", \n",
    "                 \"to_messages\", \"shared_receipt_with_poi\", \"from_this_person_to_poi\", \n",
    "                 \"from_poi_to_this_person\", \"from_messages\"]  \n",
    "print \"Features selected to be used by the Classifiers: \"\n",
    "print features_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are only 146 observations in the dataset, including potential outliers. Moreover, the classes allocation is skewed, as only 12% of the observations are PoI. It also appears that a lot of values are missing.\n",
    "\n",
    "A limited number of feature has been chosen to be used by the coming machine learning classifiers. There are several reasons for that:\n",
    "\n",
    "- A lot of values are missing. They will be replaced by 0 by the classifiers. But the features having too much missing values may not bring a lot of information to the problem.\n",
    "- Hence, I chose to use aggregated features, like total_payments or total_stock_value, which show a lower ratio of missing values.\n",
    "- Also, the feature email address is text based. To process it in the coming classifier, I should transform it into boolean, categorical or numerical value. One option could have been to determine whether the email address is from inside or outside Enron.\n",
    "- As the number of observation is limited, using too much different feature may not bring much but overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identify and remove outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOTAL    26704229\n",
      "Name: salary, dtype: object\n",
      "\n",
      "BECK SALLY W          7315\n",
      "BELDEN TIMOTHY N      7991\n",
      "KEAN STEVEN J        12754\n",
      "KITCHEN LOUISE        8305\n",
      "LAVORATO JOHN J       7259\n",
      "SHAPIRO RICHARD S    15149\n",
      "Name: to_messages, dtype: object\n",
      "\n",
      "TOTAL    32083396\n",
      "Name: deferral_payments, dtype: object\n",
      "\n",
      "LAY KENNETH L    103559793\n",
      "TOTAL            309886585\n",
      "Name: total_payments, dtype: object\n",
      "\n",
      "TOTAL    311764000\n",
      "Name: exercised_stock_options, dtype: object\n",
      "\n",
      "TOTAL    97343619\n",
      "Name: bonus, dtype: object\n",
      "\n",
      "TOTAL    130322299\n",
      "Name: restricted_stock, dtype: object\n",
      "\n",
      "BELDEN TIMOTHY N      5521\n",
      "KEAN STEVEN J         3639\n",
      "KITCHEN LOUISE        3669\n",
      "LAVORATO JOHN J       3962\n",
      "SHAPIRO RICHARD S     4527\n",
      "WHALLEY LAWRENCE G    3920\n",
      "Name: shared_receipt_with_poi, dtype: object\n",
      "\n",
      "BHATNAGAR SANJAY    15456290\n",
      "Name: restricted_stock_deferred, dtype: object\n",
      "\n",
      "TOTAL    434509511\n",
      "Name: total_stock_value, dtype: object\n",
      "\n",
      "TOTAL    5235198\n",
      "Name: expenses, dtype: object\n",
      "\n",
      "Series([], Name: loan_advances, dtype: object)\n",
      "\n",
      "BECK SALLY W            4343\n",
      "KAMINSKI WINCENTY J    14368\n",
      "KEAN STEVEN J           6759\n",
      "Name: from_messages, dtype: object\n",
      "\n",
      "LAY KENNETH L    10359729\n",
      "TOTAL            42667589\n",
      "Name: other, dtype: object\n",
      "\n",
      "BECK SALLY W        386\n",
      "DELAINEY DAVID W    609\n",
      "KEAN STEVEN J       387\n",
      "LAVORATO JOHN J     411\n",
      "Name: from_this_person_to_poi, dtype: object\n",
      "\n",
      "TOTAL    1398517\n",
      "Name: director_fees, dtype: object\n",
      "\n",
      "Series([], Name: deferred_income, dtype: object)\n",
      "\n",
      "TOTAL    48521928\n",
      "Name: long_term_incentive, dtype: object\n",
      "\n",
      "COLWELL WESLEY      240\n",
      "DIETRICH JANET R    305\n",
      "FREVERT MARK A      242\n",
      "KITCHEN LOUISE      251\n",
      "LAVORATO JOHN J     528\n",
      "Name: from_poi_to_this_person, dtype: object\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check each feature for outliers based on the 2 standard variation rule. \n",
    "for feat in df.columns:\n",
    "    try:\n",
    "        lim_h = np.mean(df[feat][df[feat] <> \"NaN\"]) + 2*np.std(df[feat][df[feat] <> \"NaN\"])\n",
    "        print df[(df[feat] > lim_h) & (df[feat] <> \"NaN\")][feat]\n",
    "        print\n",
    "    except TypeError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The previous code checked for each numerical feature which observation was lying outside of a 2 standard deviation range from the mean. This helped to recognize observations that were often lying outside of the most common values.\n",
    "\n",
    "This is how I saw that there is a \"Total\" observation include in the dataset that must aggregate part of the other observations. This line is not useful for the classifier, and may even lead it into error. Hence, it is dropped from the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bonus': 97343619,\n",
       " 'deferral_payments': 32083396,\n",
       " 'deferred_income': -27992891,\n",
       " 'director_fees': 1398517,\n",
       " 'email_address': 'NaN',\n",
       " 'exercised_stock_options': 311764000,\n",
       " 'expenses': 5235198,\n",
       " 'from_messages': 'NaN',\n",
       " 'from_poi_to_this_person': 'NaN',\n",
       " 'from_this_person_to_poi': 'NaN',\n",
       " 'loan_advances': 83925000,\n",
       " 'long_term_incentive': 48521928,\n",
       " 'other': 42667589,\n",
       " 'poi': False,\n",
       " 'restricted_stock': 130322299,\n",
       " 'restricted_stock_deferred': -7576788,\n",
       " 'salary': 26704229,\n",
       " 'shared_receipt_with_poi': 'NaN',\n",
       " 'to_messages': 'NaN',\n",
       " 'total_payments': 309886585,\n",
       " 'total_stock_value': 434509511}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove the TOTAL entry, as it appears clearcly as a general outlier. \n",
    "data_dict.pop(\"TOTAL\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create new features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 3 new features that I was willing to implement. The “to_messages_poi_ratio” feature is the ratio between the number of emails from a person to a poi and the total number of emails sent by this person. It seems to me more relevant to work with this ratio, as I would expect PoI to send more often emails to other PoIs. \n",
    "\n",
    "In the same extent, I added the “from_poi_to_this_person” and the “shared_receipt_with_poi” features which calculate similar ratios with the emails received and shared by a person.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Task 3: Create new feature(s)\n",
    "# Adding the ratio of messages received, shared and sent to POIs\n",
    "for name in data_dict.keys():\n",
    "    try:\n",
    "        data_dict[name][\"to_messages_poi_ratio\"] = \\\n",
    "            1.0*data_dict[name][\"from_this_person_to_poi\"]/data_dict[name][\"from_messages\"]\n",
    "        data_dict[name][\"from_messages_poi_ratio\"] = \\\n",
    "            1.0*data_dict[name][\"from_poi_to_this_person\"]/data_dict[name][\"to_messages\"]\n",
    "        data_dict[name][\"shared_messages_poi_ratio\"] = \\\n",
    "            1.0*data_dict[name][\"shared_receipt_with_poi\"]/data_dict[name][\"to_messages\"]\n",
    "    except TypeError:\n",
    "        data_dict[name][\"to_messages_poi_ratio\"] = \"NaN\"\n",
    "        data_dict[name][\"from_messages_poi_ratio\"] = \"NaN\"\n",
    "        data_dict[name][\"shared_messages_poi_ratio\"] = \"NaN\"\n",
    "\n",
    "features_list.append(\"to_messages_poi_ratio\")\n",
    "features_list.append(\"from_messages_poi_ratio\")\n",
    "features_list.append(\"shared_messages_poi_ratio\")\n",
    "        \n",
    "# Store to my_dataset for easy export below.\n",
    "my_dataset = data_dict\n",
    "\n",
    "# Extract features and labels from dataset for local testing\n",
    "data = featureFormat(my_dataset, features_list, sort_keys=True)\n",
    "labels, features = targetFeatureSplit(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification performance with the initial feature list: \n",
      "['poi', 'salary', 'total_payments', 'total_stock_value', 'to_messages', 'shared_receipt_with_poi', 'from_this_person_to_poi', 'from_poi_to_this_person', 'from_messages']\n",
      "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=42, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)\n",
      "\tAccuracy: 0.62793\tPrecision: 0.04172\tRecall: 0.08150\tF1: 0.05519\tF2: 0.06845\n",
      "\tTotal predictions: 15000\tTrue positives:  163\tFalse positives: 3744\tFalse negatives: 1837\tTrue negatives: 9256\n",
      "\n",
      "Classification performance with the extended feature list: \n",
      "['poi', 'salary', 'total_payments', 'total_stock_value', 'to_messages', 'shared_receipt_with_poi', 'from_this_person_to_poi', 'from_poi_to_this_person', 'from_messages', 'to_messages_poi_ratio', 'from_messages_poi_ratio', 'shared_messages_poi_ratio']\n",
      "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=42, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)\n",
      "\tAccuracy: 0.62880\tPrecision: 0.04044\tRecall: 0.07850\tF1: 0.05338\tF2: 0.06607\n",
      "\tTotal predictions: 15000\tTrue positives:  157\tFalse positives: 3725\tFalse negatives: 1843\tTrue negatives: 9275\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# initial feature list to be compared with the extended one\n",
    "features_list_init = ['poi', 'salary', 'total_payments', \"total_stock_value\", \n",
    "                 \"to_messages\", \"shared_receipt_with_poi\", \"from_this_person_to_poi\", \n",
    "                 \"from_poi_to_this_person\", \"from_messages\"]  \n",
    "\n",
    "# Using the standard Logistic Regression classifier to study the impact of adding these new features\n",
    "clf = LogisticRegression(random_state=42)\n",
    "print \"Classification performance with the initial feature list: \"\n",
    "print features_list_init\n",
    "test_classifier(clf, my_dataset, features_list_init)\n",
    "\n",
    "print \"Classification performance with the extended feature list: \"\n",
    "print features_list\n",
    "test_classifier(clf, my_dataset, features_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apparently, adding these new features does not seem to provide a lot more of usable information. But tuning the hyper parameters may increase the usefulness of these added features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Trying some basic estimators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standard SVC, Random Forest and Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing:  SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape=None, degree=3, gamma='auto', kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False)\n",
      "true negatives:  13000\n",
      "false negatives:  2000\n",
      "false positives:  0\n",
      "true positives:  0\n",
      "Got a divide by zero when trying out: SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape=None, degree=3, gamma='auto', kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False)\n",
      "Precision or recall may be undefined due to a lack of true positive predicitons.\n",
      "\n",
      "testing:  RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False)\n",
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False)\n",
      "\tAccuracy: 0.85007\tPrecision: 0.32189\tRecall: 0.11250\tF1: 0.16673\tF2: 0.12933\n",
      "\tTotal predictions: 15000\tTrue positives:  225\tFalse positives:  474\tFalse negatives: 1775\tTrue negatives: 12526\n",
      "\n",
      "\n",
      "testing:  LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)\n",
      "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)\n",
      "\tAccuracy: 0.62880\tPrecision: 0.04044\tRecall: 0.07850\tF1: 0.05338\tF2: 0.06607\n",
      "\tTotal predictions: 15000\tTrue positives:  157\tFalse positives: 3725\tFalse negatives: 1843\tTrue negatives: 9275\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for clf in [SVC(), RandomForestClassifier(), LogisticRegression()]:\n",
    "    print \"testing: \", clf\n",
    "    # test the classifier with the testing function provided\n",
    "    # a 1000-folds cross-validation is done and different metrics are calculated (accuracy, precision, recall, custom f-scores)\n",
    "    # to assess the classifier performance.\n",
    "    test_classifier(clf, my_dataset, features_list)\n",
    "    print\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the standard classification algorithms, without scaling, dimensionality reduction ,or hyper parameter tuning does not provide a high level of performance.\n",
    "\n",
    "The Random Forest classifier gives the best results, but as it is an ensemble classifier, it is better suited to tackle overfitting per default. \n",
    "\n",
    "The SVC classifier is not well performing as it classifies all the users as non-PoI. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection with Random Forest, Standard SVC, Random Forest and Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature importance higher than 10%\n",
      "0   salary   0.138607693197\n",
      "2   total_stock_value   0.204590091907\n",
      "6   from_poi_to_this_person   0.10834610424\n",
      "8   to_messages_poi_ratio   0.110092706325\n",
      "10   shared_messages_poi_ratio   0.134299919318\n",
      "\n",
      "testing:  SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape=None, degree=3, gamma='auto', kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False)\n",
      "true negatives:  12000\n",
      "false negatives:  2000\n",
      "false positives:  0\n",
      "true positives:  0\n",
      "Got a divide by zero when trying out: SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape=None, degree=3, gamma='auto', kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False)\n",
      "Precision or recall may be undefined due to a lack of true positive predicitons.\n",
      "\n",
      "testing:  RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False)\n",
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False)\n",
      "\tAccuracy: 0.83943\tPrecision: 0.36752\tRecall: 0.17200\tF1: 0.23433\tF2: 0.19248\n",
      "\tTotal predictions: 14000\tTrue positives:  344\tFalse positives:  592\tFalse negatives: 1656\tTrue negatives: 11408\n",
      "\n",
      "\n",
      "testing:  LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)\n",
      "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)\n",
      "\tAccuracy: 0.58107\tPrecision: 0.04816\tRecall: 0.10300\tF1: 0.06564\tF2: 0.08390\n",
      "\tTotal predictions: 14000\tTrue positives:  206\tFalse positives: 4071\tFalse negatives: 1794\tTrue negatives: 7929\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Choose the most important features with a random forest classifier\n",
    "clf = RandomForestClassifier(random_state=42)\n",
    "clf.fit(features, labels)\n",
    "\n",
    "print \"feature importance higher than 10%\"\n",
    "for i in range(len(clf.feature_importances_)):\n",
    "    if clf.feature_importances_[i] > 0.1:\n",
    "        print i, \" \", features_list[i+1], \" \", clf.feature_importances_[i]\n",
    "print\n",
    "\n",
    "\n",
    "features_list_selected = ['poi', 'salary', \"total_stock_value\", \n",
    "                          \"from_poi_to_this_person\", \"to_messages_poi_ratio\",\n",
    "                          \"shared_messages_poi_ratio\"] \n",
    "\n",
    "\n",
    "for clf in [SVC(), RandomForestClassifier(), LogisticRegression()]:\n",
    "    print \"testing: \", clf\n",
    "    # test the classifier with the testing function provided\n",
    "    # a 1000-folds cross-validation is done and different metrics are calculated (accuracy, precision, recall, custom f-scores)\n",
    "    # to assess the classifier performance.\n",
    "    test_classifier(clf, my_dataset, features_list_selected)\n",
    "    print\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I used the Random Forest classifier to determine the most useful features in order to proceed to a manual feature selection. I only kept the features providing more than 10% of importance.\n",
    "\n",
    "Using this feature selection slightly increases the results for the standard random forest classifier and the logistic regression classifier.  \n",
    "\n",
    "One solution would be to move further with these selected features, but, I preferred to use next a dimensionality reduction approach with Principal Component Analysis to tackle the overfitting risk. \n",
    "\n",
    "The SVC classifier is still not well performing as it classifies all the users as non-PoI.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning\n",
    "\n",
    "In the following code chunks, I try to optimize hyper parameters of different classification algorithms using the Grid Search function from the Sickit Learn package. \n",
    "\n",
    "The Grid Search approach trains a classifier for each hyper parameter combination of values and conducts a 10-folds cross-validation on the provided data set to select the best estimator.  I used then the test_classifier function from the tester.py file to calculate the classifier performance.\n",
    "\n",
    "In addition to optimizing the Classification algorithm itself, I also work on pipeline estimators that also include a MinMax Scaler and a PCA transformation to increase the classification algorithm performance.\n",
    "\n",
    "### Grid Search Optimization: SVC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting the classifier to the training set\n",
      "done in 2.228s\n",
      "Best estimator found by grid search:\n",
      "SVC(C=1.0, cache_size=200, class_weight='balanced', coef0=0.0,\n",
      "  decision_function_shape=None, degree=3, gamma=0.001, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False)\n",
      "true negatives:  13000\n",
      "false negatives:  2000\n",
      "false positives:  0\n",
      "true positives:  0\n",
      "Got a divide by zero when trying out: SVC(C=1.0, cache_size=200, class_weight='balanced', coef0=0.0,\n",
      "  decision_function_shape=None, degree=3, gamma=0.001, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False)\n",
      "Precision or recall may be undefined due to a lack of true positive predicitons.\n"
     ]
    }
   ],
   "source": [
    "print \"Fitting the classifier to the training set\"\n",
    "t0 = time()\n",
    "\n",
    "# the linear and polynomial kernels are not included here. After some first tests, the test_classifier function \n",
    "# or the grid search algorythm gets stuck. Without PCA, the classification may not be done properly with linear kernels.\n",
    "# The polynomial kernel was not working either with the PCA\n",
    "\n",
    "param_grid = {\n",
    "    'kernel':['sigmoid', 'rbf'], \n",
    "    'C': np.logspace(-3, 3, 7),\n",
    "    'gamma':np.logspace(-3, 3, 7)\n",
    "          }\n",
    "\n",
    "clf = GridSearchCV(SVC(class_weight='balanced'), param_grid, scoring=scorer_f, cv=10)\n",
    "clf = clf.fit(features, labels)\n",
    "print \"done in %0.3fs\" % (time() - t0)\n",
    "print \"Best estimator found by grid search:\"\n",
    "print clf.best_estimator_\n",
    "test_classifier(clf.best_estimator_, my_dataset, features_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid Search Optimization: Scaling, PCA and SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting the classifier to the training set\n",
      "done in 98.214s\n",
      "Best estimator found by grid search:\n",
      "Pipeline(steps=[('scaler', MinMaxScaler(copy=True, feature_range=(0, 1))), ('pca', RandomizedPCA(copy=True, iterated_power=3, n_components=6, random_state=None,\n",
      "       whiten=True)), ('svc', SVC(C=100.0, cache_size=200, class_weight='balanced', coef0=0.0,\n",
      "  decision_function_shape=None, degree=2, gamma=0.001, kernel='linear',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False))])\n",
      "Testing the best estimator\n",
      "Pipeline(steps=[('scaler', MinMaxScaler(copy=True, feature_range=(0, 1))), ('pca', RandomizedPCA(copy=True, iterated_power=3, n_components=6, random_state=None,\n",
      "       whiten=True)), ('svc', SVC(C=100.0, cache_size=200, class_weight='balanced', coef0=0.0,\n",
      "  decision_function_shape=None, degree=2, gamma=0.001, kernel='linear',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False))])\n",
      "\tAccuracy: 0.72013\tPrecision: 0.26036\tRecall: 0.59700\tF1: 0.36259\tF2: 0.47434\n",
      "\tTotal predictions: 15000\tTrue positives: 1194\tFalse positives: 3392\tFalse negatives:  806\tTrue negatives: 9608\n",
      "\n",
      "done in 13.206s\n"
     ]
    }
   ],
   "source": [
    "estimators = [\n",
    "    ('scaler',MinMaxScaler()),\n",
    "    ('pca', RandomizedPCA(whiten=True)), \n",
    "    ('svc', SVC(class_weight=\"balanced\", degree=2))]\n",
    "pipe = Pipeline(estimators)\n",
    "\n",
    "param_grid = {\n",
    "    'pca__n_components' : [4, 6, 10, 15, 20],\n",
    "    'svc__kernel':['linear', 'sigmoid'], \n",
    "    'svc__C': np.logspace(-3, 3, 7),\n",
    "    'svc__gamma':np.logspace(-3, 3, 7)\n",
    "             }\n",
    "\n",
    "#    'svc__kernel':['linear', 'sigmoid', 'rbf', 'poly'], \n",
    "clf = GridSearchCV(pipe, param_grid, scoring=scorer_f, cv=10)\n",
    "\n",
    "print \"Fitting the classifier to the training set\"\n",
    "t0 = time()\n",
    "clf.fit(features, labels)\n",
    "print \"done in %0.3fs\" % (time() - t0)\n",
    "print \"Best estimator found by grid search:\"\n",
    "print clf.best_estimator_\n",
    "\n",
    "print \"Testing the best estimator\"\n",
    "t0 = time()\n",
    "test_classifier(clf.best_estimator_, my_dataset, features_list)\n",
    "print \"done in %0.3fs\" % (time() - t0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid Search Optimization: Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting the classifier to the training set\n",
      "done in 64.368s\n",
      "Best estimator found by grid search:\n",
      "RandomForestClassifier(bootstrap=True, class_weight='balanced',\n",
      "            criterion='gini', max_depth=None, max_features='auto',\n",
      "            max_leaf_nodes=None, min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=15, n_jobs=-1,\n",
      "            oob_score=False, random_state=42, verbose=0, warm_start=False)\n",
      "Testing the best estimator\n",
      "RandomForestClassifier(bootstrap=True, class_weight='balanced',\n",
      "            criterion='gini', max_depth=None, max_features='auto',\n",
      "            max_leaf_nodes=None, min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=15, n_jobs=-1,\n",
      "            oob_score=False, random_state=42, verbose=0, warm_start=False)\n",
      "\tAccuracy: 0.85073\tPrecision: 0.34541\tRecall: 0.13350\tF1: 0.19257\tF2: 0.15217\n",
      "\tTotal predictions: 15000\tTrue positives:  267\tFalse positives:  506\tFalse negatives: 1733\tTrue negatives: 12494\n",
      "\n",
      "done in 307.026s\n"
     ]
    }
   ],
   "source": [
    "print \"Fitting the classifier to the training set\"\n",
    "t0 = time()\n",
    "param_grid = {\n",
    "         'n_estimators': [1, 5, 10, 15, 20],\n",
    "         'min_samples_leaf' : [1, 2, 3] \n",
    "          }\n",
    "\n",
    "clf = GridSearchCV(RandomForestClassifier(class_weight='balanced', random_state=42, n_jobs=-1), \n",
    "                   param_grid, scoring=scorer_f, cv=10)\n",
    "clf = clf.fit(features, labels)\n",
    "print \"done in %0.3fs\" % (time() - t0)\n",
    "print \"Best estimator found by grid search:\"\n",
    "print clf.best_estimator_\n",
    "\n",
    "print \"Testing the best estimator\"\n",
    "t0 = time()\n",
    "test_classifier(clf.best_estimator_, my_dataset, features_list)\n",
    "print \"done in %0.3fs\" % (time() - t0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid Search Optimization: Scaling, PCA & Random Forest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting the classifier to the training set\n",
      "done in 269.658s\n",
      "Best estimator found by grid search:\n",
      "Pipeline(steps=[('scaler', MinMaxScaler(copy=True, feature_range=(0, 1))), ('pca', RandomizedPCA(copy=True, iterated_power=3, n_components=15, random_state=None,\n",
      "       whiten=True)), ('randomforest', RandomForestClassifier(bootstrap=True, class_weight='balanced',\n",
      "            criterion='gini', max_depth=None...timators=15, n_jobs=-1,\n",
      "            oob_score=False, random_state=42, verbose=0, warm_start=False))])\n",
      "Testing the best estimator\n",
      "Pipeline(steps=[('scaler', MinMaxScaler(copy=True, feature_range=(0, 1))), ('pca', RandomizedPCA(copy=True, iterated_power=3, n_components=15, random_state=None,\n",
      "       whiten=True)), ('randomforest', RandomForestClassifier(bootstrap=True, class_weight='balanced',\n",
      "            criterion='gini', max_depth=None...timators=15, n_jobs=-1,\n",
      "            oob_score=False, random_state=42, verbose=0, warm_start=False))])\n",
      "\tAccuracy: 0.85400\tPrecision: 0.26601\tRecall: 0.05400\tF1: 0.08978\tF2: 0.06424\n",
      "\tTotal predictions: 15000\tTrue positives:  108\tFalse positives:  298\tFalse negatives: 1892\tTrue negatives: 12702\n",
      "\n",
      "done in 259.008s\n"
     ]
    }
   ],
   "source": [
    "estimators = [\n",
    "    ('scaler',MinMaxScaler()),\n",
    "    ('pca', RandomizedPCA(whiten=True)), \n",
    "    ('randomforest', RandomForestClassifier(class_weight='balanced', random_state=42, n_jobs=-1))]\n",
    "pipe = Pipeline(estimators)\n",
    "\n",
    "param_grid = {\n",
    "    'pca__n_components':[4, 6, 10, 15, 20],\n",
    "    'randomforest__n_estimators':[1, 5, 10, 15, 20],\n",
    "    'randomforest__min_samples_leaf':[1, 2, 3] \n",
    "             }\n",
    "clf = GridSearchCV(pipe, param_grid, scoring=scorer_f, cv=10)\n",
    "\n",
    "print \"Fitting the classifier to the training set\"\n",
    "t0 = time()\n",
    "clf = clf.fit(features, labels)\n",
    "print \"done in %0.3fs\" % (time() - t0)\n",
    "print \"Best estimator found by grid search:\"\n",
    "print clf.best_estimator_\n",
    "\n",
    "print \"Testing the best estimator\"\n",
    "t0 = time()\n",
    "test_classifier(clf.best_estimator_, my_dataset, features_list)\n",
    "print \"done in %0.3fs\" % (time() - t0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid Search Optimization: Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting the classifier to the training set\n",
      "done in 0.847s\n",
      "Best estimator found by grid search:\n",
      "LogisticRegression(C=10.0, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='ovr', n_jobs=1, penalty='l1', random_state=42,\n",
      "          solver='liblinear', tol=0.0001, verbose=0, warm_start=False)\n",
      "Testing the best estimator\n",
      "LogisticRegression(C=10.0, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='ovr', n_jobs=1, penalty='l1', random_state=42,\n",
      "          solver='liblinear', tol=0.0001, verbose=0, warm_start=False)\n",
      "\tAccuracy: 0.81593\tPrecision: 0.38310\tRecall: 0.62350\tF1: 0.47460\tF2: 0.55398\n",
      "\tTotal predictions: 15000\tTrue positives: 1247\tFalse positives: 2008\tFalse negatives:  753\tTrue negatives: 10992\n",
      "\n",
      "done in 1.897s\n"
     ]
    }
   ],
   "source": [
    "print \"Fitting the classifier to the training set\"\n",
    "t0 = time()\n",
    "param_grid = {\n",
    "         'C': np.logspace(-7, 7, 15),\n",
    "         'penalty' : [\"l1\", \"l2\"] \n",
    "          }\n",
    "\n",
    "\n",
    "clf = GridSearchCV(LogisticRegression(class_weight=\"balanced\", random_state=42), \n",
    "                   param_grid, scoring=scorer_f, cv=10)\n",
    "clf = clf.fit(features, labels)\n",
    "print \"done in %0.3fs\" % (time() - t0)\n",
    "print \"Best estimator found by grid search:\"\n",
    "print clf.best_estimator_\n",
    "\n",
    "print \"Testing the best estimator\"\n",
    "t0 = time()\n",
    "test_classifier(clf.best_estimator_, my_dataset, features_list)\n",
    "print \"done in %0.3fs\" % (time() - t0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Grid Search Optimization: Feature scaling, PCA and Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting the classifier to the training set\n",
      "done in 10.319s\n",
      "Best estimator found by grid search:\n",
      "Pipeline(steps=[('scaler', MinMaxScaler(copy=True, feature_range=(0, 1))), ('pca', RandomizedPCA(copy=True, iterated_power=3, n_components=15, random_state=None,\n",
      "       whiten=True)), ('logreg', LogisticRegression(C=1.0, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='ovr', n_jobs=-1, penalty='l2', random_state=42,\n",
      "          solver='liblinear', tol=0.0001, verbose=0, warm_start=False))])\n",
      "Testing the best estimator\n",
      "Pipeline(steps=[('scaler', MinMaxScaler(copy=True, feature_range=(0, 1))), ('pca', RandomizedPCA(copy=True, iterated_power=3, n_components=15, random_state=None,\n",
      "       whiten=True)), ('logreg', LogisticRegression(C=1.0, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='ovr', n_jobs=-1, penalty='l2', random_state=42,\n",
      "          solver='liblinear', tol=0.0001, verbose=0, warm_start=False))])\n",
      "\tAccuracy: 0.75793\tPrecision: 0.27906\tRecall: 0.51500\tF1: 0.36198\tF2: 0.44051\n",
      "\tTotal predictions: 15000\tTrue positives: 1030\tFalse positives: 2661\tFalse negatives:  970\tTrue negatives: 10339\n",
      "\n",
      "done in 1.678s\n"
     ]
    }
   ],
   "source": [
    "estimators = [('scaler',MinMaxScaler()),\n",
    "              ('pca', RandomizedPCA(whiten=True)), \n",
    "              ('logreg', LogisticRegression(class_weight=\"balanced\", random_state=42, n_jobs=-1))]\n",
    "pipe = Pipeline(estimators)\n",
    "\n",
    "param_grid = {\n",
    "    'pca__n_components' : [4, 6, 10, 15, 20],\n",
    "    'logreg__C': np.logspace(-7, 7, 15),\n",
    "    'logreg__penalty' : [\"l1\", \"l2\"] \n",
    "             }\n",
    "clf = GridSearchCV(pipe, param_grid, scoring=scorer_f, cv=10)\n",
    "\n",
    "print \"Fitting the classifier to the training set\"\n",
    "t0 = time()\n",
    "clf.fit(features, labels)\n",
    "print \"done in %0.3fs\" % (time() - t0)\n",
    "print \"Best estimator found by grid search:\"\n",
    "print clf.best_estimator_\n",
    "\n",
    "print \"Testing the best estimator\"\n",
    "t0 = time()\n",
    "test_classifier(clf.best_estimator_, my_dataset, features_list)\n",
    "print \"done in %0.3fs\" % (time() - t0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "In general, scaling the features and reducing their dimensionality should help to increase the classifier performance. \n",
    "\n",
    "But to my surprise, the best classification performance is provided by the simple optimized Logistic Regression. Moreover, this algorithm is really fast to train than the SVC and the Random Forest.\n",
    "\n",
    "I suspect that, as the C hyper parameter from the logistic regression algorithm has been developed to tackle the issue of overfitting, it may mitigate the need to use a dimensionality reduction approach. It also confirms that logistic regression does not need scaling to be properly trained. \n",
    "\n",
    "As a consequence, I decided to implement this optimized algorithm in the poi_id file. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bibliography\n",
    "\n",
    "\n",
    "- Sickit Learn examples : http://scikit-learn.org/ (RandomForestClassifier, AdaBoostClassifier, LogisticRegression, Pipeline, GridSearchCV, StratifiedShuffleSplit, RandomizedPCA, SVC)\n",
    "- Python Machine Learning, Sebastian Raschka, 2015, Packt Publishing\n",
    "- Udacity Nanoprogram - Machine Learning Course"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
