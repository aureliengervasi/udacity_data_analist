{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# P5 - Identify Fraud from Enron Email\n",
    "\n",
    "## Motivation\n",
    "\n",
    "This work is part of the Udacity Data Analyst Nanoprogram. This is the final project for the Machine Learning course. The goal of this project is to use machine learning algorithms to build an efficient classifier that can spot PoI (Person of Interest) in the Enron Dataset provided by the Udacity team.\n",
    "\n",
    "## Dataset\n",
    "\n",
    "The dataset used in this project has been extracted by the Udacity team from the huge Enron dataset. The original Enron dataset contains data from about 150 users, mostly senior management of Enron, organized into folders. The corpus contains a total of about 500 000 messages. This data was originally made public, and posted to the web, by the Federal Energy Regulatory Commission during its investigation. \n",
    "\n",
    "https://www.cs.cmu.edu/~./enron/\n",
    "\n",
    "The Udacity team extracted some useful features related to email correspondence for 145 users and enhanced it with financial data for these 145 users (salary, stock options, etc.).\n",
    "\n",
    "## Summary\n",
    "\n",
    "The first part is dedicated to analyze the features, get insights about the dataset, and spot the main outliers. Some feature engineering is done by adding 3 new features built out of existing ones. Their impact on the classifier performance seems to be positive. \n",
    "\n",
    "Tree-based feature selection is implemented to reduce the number of features and limit the risk of overfitting. When calculating the impact of this selection on a RandomForest Classifier, it appears that the performance gain is also limited. As a consequence, the entire feature list is used in the rest of the work, combined with a Principal Component Analysis to reduce its dimensionality. \n",
    "\n",
    "Different basic machine learning algorithms are then tested, with limited success. These algorithms are very dependent from hyper parameters. These can for instance play a strong role on overfitting. In order to determine the best parameters for this Enron Dataset, a Grid Search Analysis is done on different Classifiers. The Grid Search Analysis is done using a f-score as scorer similar to the one calculated by the tester.py file.\n",
    "\n",
    "In order to take the best out of these Classifiers, feature preparation is integrated in a pipeline estimator. More specifically, features are first scaled with a Standard Scaler, then their dimension is reduced with a Principal Component Analysis before supplying them to the Classification Algorithm. \n",
    "\n",
    "Each classifier performance is calculated with the test_classifier function from the tester.py file. This function conducts a 1000-folds cross validation with the given classifier and the given feature list. It prints out different performance metrics (accuracy, precision, recall, f-scores) that help us assess the Classifier performance.\n",
    "\n",
    "In the end, the best classifier that pops out is an optimized Logistic Regression Classifier using a L1 distance penalty. This is the algorithm that has been implemented in my poi_id.py file. \n",
    "\n",
    "## Limitations and potentials\n",
    "\n",
    "One of the major limitation of this work was the dataset size and the numurous missing data it contains. There were only 145 usable observations to build a classifier and most of them were missing values for one or more features. This increased a lot the risk of overfitting. \n",
    "\n",
    "In order to tackle this issue, the StratifiedShuffleSplit function from the sickit learn cross-validation package was used to implement 10-folds to 1000-folds cross validation when developing and testing the different algorithms. \n",
    "\n",
    "In order to improve further the classifier performance, it would be interesting to try to implement some specific ensemble classifiers combining for example logistic regression with SVC or decision trees. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import pickle\n",
    "from time import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tester import dump_classifier_and_data, test_classifier\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cross_validation import StratifiedShuffleSplit\n",
    "from sklearn.decomposition import RandomizedPCA\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "sys.path.append(\"../tools/\")\n",
    "from feature_format import featureFormat, targetFeatureSplit\n",
    "\n",
    "def scorer_f(clf, X, y):\n",
    "    '''\n",
    "    This function is used to evaluate the f-score from the tester.py file of a given classifier \n",
    "    on a given feature & label dataset. It is used in the grid search algorythm to select the best fit.\n",
    "    The code is extracted from the tester.py file.\n",
    "    '''\n",
    "    true_negatives = 0\n",
    "    false_negatives = 0\n",
    "    true_positives = 0\n",
    "    false_positives = 0\n",
    "    clf.fit(X, y)\n",
    "    predictions = clf.predict(X)\n",
    "    for prediction, truth in zip(predictions, y):\n",
    "        if prediction == 0 and truth == 0:\n",
    "            true_negatives += 1\n",
    "        elif prediction == 0 and truth == 1:\n",
    "            false_negatives += 1\n",
    "        elif prediction == 1 and truth == 0:\n",
    "            false_positives += 1\n",
    "        elif prediction == 1 and truth == 1:\n",
    "            true_positives += 1\n",
    "    try:\n",
    "        precision = 1.0*true_positives/(true_positives+false_positives)\n",
    "        recall = 1.0*true_positives/(true_positives+false_negatives)\n",
    "        f1 = 2.0 * true_positives/(2*true_positives + false_positives+false_negatives)\n",
    "        f2 = (1+2.0*2.0) * precision*recall/(4*precision + recall)\n",
    "        return f2\n",
    "    except ZeroDivisionError:\n",
    "        # print \"Got a divide by zero when trying out:\", clf\n",
    "        # print \"Precision or recall may be undefined due to a lack of true positive predicitons.\"\n",
    "        return 0\n",
    "\n",
    "# Load the dictionary containing the dataset\n",
    "with open(\"final_project_dataset.pkl\", \"r\") as data_file:\n",
    "    data_dict = pickle.load(data_file)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    salary to_messages deferral_payments total_payments  \\\n",
      "ALLEN PHILLIP K     201955        2902           2869717        4484442   \n",
      "BADUM JAMES P          NaN         NaN            178980         182466   \n",
      "BANNANTINE JAMES M     477         566               NaN         916197   \n",
      "BAXTER JOHN C       267102         NaN           1295738        5634343   \n",
      "BAY FRANKLIN R      239671         NaN            260455         827696   \n",
      "\n",
      "                   exercised_stock_options    bonus restricted_stock  \\\n",
      "ALLEN PHILLIP K                    1729541  4175000           126027   \n",
      "BADUM JAMES P                       257817      NaN              NaN   \n",
      "BANNANTINE JAMES M                 4046157      NaN          1757552   \n",
      "BAXTER JOHN C                      6680544  1200000          3942714   \n",
      "BAY FRANKLIN R                         NaN   400000           145796   \n",
      "\n",
      "                   shared_receipt_with_poi restricted_stock_deferred  \\\n",
      "ALLEN PHILLIP K                       1407                   -126027   \n",
      "BADUM JAMES P                          NaN                       NaN   \n",
      "BANNANTINE JAMES M                     465                   -560222   \n",
      "BAXTER JOHN C                          NaN                       NaN   \n",
      "BAY FRANKLIN R                         NaN                    -82782   \n",
      "\n",
      "                   total_stock_value           ...           loan_advances  \\\n",
      "ALLEN PHILLIP K              1729541           ...                     NaN   \n",
      "BADUM JAMES P                 257817           ...                     NaN   \n",
      "BANNANTINE JAMES M           5243487           ...                     NaN   \n",
      "BAXTER JOHN C               10623258           ...                     NaN   \n",
      "BAY FRANKLIN R                 63014           ...                     NaN   \n",
      "\n",
      "                   from_messages    other from_this_person_to_poi    poi  \\\n",
      "ALLEN PHILLIP K             2195      152                      65  False   \n",
      "BADUM JAMES P                NaN      NaN                     NaN  False   \n",
      "BANNANTINE JAMES M            29   864523                       0  False   \n",
      "BAXTER JOHN C                NaN  2660303                     NaN  False   \n",
      "BAY FRANKLIN R               NaN       69                     NaN  False   \n",
      "\n",
      "                   director_fees deferred_income long_term_incentive  \\\n",
      "ALLEN PHILLIP K              NaN        -3081055              304805   \n",
      "BADUM JAMES P                NaN             NaN                 NaN   \n",
      "BANNANTINE JAMES M           NaN           -5104                 NaN   \n",
      "BAXTER JOHN C                NaN        -1386055             1586055   \n",
      "BAY FRANKLIN R               NaN         -201641                 NaN   \n",
      "\n",
      "                                 email_address from_poi_to_this_person  \n",
      "ALLEN PHILLIP K        phillip.allen@enron.com                      47  \n",
      "BADUM JAMES P                              NaN                     NaN  \n",
      "BANNANTINE JAMES M  james.bannantine@enron.com                      39  \n",
      "BAXTER JOHN C                              NaN                     NaN  \n",
      "BAY FRANKLIN R             frank.bay@enron.com                     NaN  \n",
      "\n",
      "[5 rows x 21 columns]\n",
      "\n",
      "Total number of observations (including potential outliers):  146\n",
      "\n",
      "Number of PoI: 18 (12.3%)\n",
      "\n",
      "Total number of features:  20\n",
      "\n",
      "Number of Nan values for salary: 51 (34.9%)\n",
      "Number of Nan values for to_messages: 60 (41.1%)\n",
      "Number of Nan values for deferral_payments: 107 (73.3%)\n",
      "Number of Nan values for total_payments: 21 (14.4%)\n",
      "Number of Nan values for exercised_stock_options: 44 (30.1%)\n",
      "Number of Nan values for bonus: 64 (43.8%)\n",
      "Number of Nan values for restricted_stock: 36 (24.7%)\n",
      "Number of Nan values for shared_receipt_with_poi: 60 (41.1%)\n",
      "Number of Nan values for restricted_stock_deferred: 128 (87.7%)\n",
      "Number of Nan values for total_stock_value: 20 (13.7%)\n",
      "Number of Nan values for expenses: 51 (34.9%)\n",
      "Number of Nan values for loan_advances: 142 (97.3%)\n",
      "Number of Nan values for from_messages: 60 (41.1%)\n",
      "Number of Nan values for other: 53 (36.3%)\n",
      "Number of Nan values for from_this_person_to_poi: 60 (41.1%)\n",
      "Number of Nan values for director_fees: 129 (88.4%)\n",
      "Number of Nan values for deferred_income: 97 (66.4%)\n",
      "Number of Nan values for long_term_incentive: 80 (54.8%)\n",
      "Number of Nan values for email_address: 35 (24.0%)\n",
      "Number of Nan values for from_poi_to_this_person: 60 (41.1%)\n",
      "\n",
      "Features selected to be used by the classifiers: \n",
      "['poi', 'salary', 'to_messages', 'deferral_payments', 'total_payments', 'exercised_stock_options', 'bonus', 'restricted_stock', 'shared_receipt_with_poi', 'restricted_stock_deferred', 'total_stock_value', 'expenses', 'loan_advances', 'from_messages', 'other', 'from_this_person_to_poi', 'director_fees', 'deferred_income', 'long_term_incentive', 'from_poi_to_this_person']\n"
     ]
    }
   ],
   "source": [
    "# Import dataset into a pandas dataframe to easily manipulate the data\n",
    "df = pd. DataFrame.from_dict(data_dict, orient=\"index\")  \n",
    "print df.head(5) \n",
    "print \n",
    "\n",
    "# Total number of points\n",
    "print \"Total number of observations (including potential outliers): \", len(df)\n",
    "print\n",
    "\n",
    "\n",
    "# Allocation accross classes\n",
    "print \"Number of PoI: {0} ({1:.1f}%)\".format(sum(df.poi), 100.0*sum(df.poi)/len(df.poi))\n",
    "print\n",
    "\n",
    "# Total nubmer of features\n",
    "print \"Total number of features: \", len(df.columns)-1\n",
    "print\n",
    "\n",
    "# Missing values\n",
    "for feat in df.columns:\n",
    "    try:\n",
    "        n_NaN = sum(df[feat]==\"NaN\")\n",
    "        print \"Number of Nan values for {0}: {1} ({2:.1f}%)\".format(feat, n_NaN, 100.0* n_NaN/len(df.poi) )\n",
    "    except TypeError:\n",
    "        pass\n",
    "print\n",
    "\n",
    "# features_list is a list of strings, each of which is a feature name.\n",
    "# The first feature must be \"poi\".\n",
    "\n",
    "features_list = ['poi']\n",
    "for feat in df.columns:\n",
    "    if feat not in ['poi', 'email_address']:\n",
    "        features_list.append(feat)\n",
    "\n",
    "print \"Features selected to be used by the classifiers: \"\n",
    "print features_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are only 146 observations in the dataset, including potential outliers. Moreover, the classes allocation is skewed, as only 12% of the observations are PoI. It also appears that a lot of values are missing.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identify and remove outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOTAL    26704229\n",
      "Name: salary, dtype: object\n",
      "\n",
      "BECK SALLY W          7315\n",
      "BELDEN TIMOTHY N      7991\n",
      "KEAN STEVEN J        12754\n",
      "KITCHEN LOUISE        8305\n",
      "LAVORATO JOHN J       7259\n",
      "SHAPIRO RICHARD S    15149\n",
      "Name: to_messages, dtype: object\n",
      "\n",
      "TOTAL    32083396\n",
      "Name: deferral_payments, dtype: object\n",
      "\n",
      "LAY KENNETH L    103559793\n",
      "TOTAL            309886585\n",
      "Name: total_payments, dtype: object\n",
      "\n",
      "TOTAL    311764000\n",
      "Name: exercised_stock_options, dtype: object\n",
      "\n",
      "TOTAL    97343619\n",
      "Name: bonus, dtype: object\n",
      "\n",
      "TOTAL    130322299\n",
      "Name: restricted_stock, dtype: object\n",
      "\n",
      "BELDEN TIMOTHY N      5521\n",
      "KEAN STEVEN J         3639\n",
      "KITCHEN LOUISE        3669\n",
      "LAVORATO JOHN J       3962\n",
      "SHAPIRO RICHARD S     4527\n",
      "WHALLEY LAWRENCE G    3920\n",
      "Name: shared_receipt_with_poi, dtype: object\n",
      "\n",
      "BHATNAGAR SANJAY    15456290\n",
      "Name: restricted_stock_deferred, dtype: object\n",
      "\n",
      "TOTAL    434509511\n",
      "Name: total_stock_value, dtype: object\n",
      "\n",
      "TOTAL    5235198\n",
      "Name: expenses, dtype: object\n",
      "\n",
      "Series([], Name: loan_advances, dtype: object)\n",
      "\n",
      "BECK SALLY W            4343\n",
      "KAMINSKI WINCENTY J    14368\n",
      "KEAN STEVEN J           6759\n",
      "Name: from_messages, dtype: object\n",
      "\n",
      "LAY KENNETH L    10359729\n",
      "TOTAL            42667589\n",
      "Name: other, dtype: object\n",
      "\n",
      "BECK SALLY W        386\n",
      "DELAINEY DAVID W    609\n",
      "KEAN STEVEN J       387\n",
      "LAVORATO JOHN J     411\n",
      "Name: from_this_person_to_poi, dtype: object\n",
      "\n",
      "TOTAL    1398517\n",
      "Name: director_fees, dtype: object\n",
      "\n",
      "Series([], Name: deferred_income, dtype: object)\n",
      "\n",
      "TOTAL    48521928\n",
      "Name: long_term_incentive, dtype: object\n",
      "\n",
      "COLWELL WESLEY      240\n",
      "DIETRICH JANET R    305\n",
      "FREVERT MARK A      242\n",
      "KITCHEN LOUISE      251\n",
      "LAVORATO JOHN J     528\n",
      "Name: from_poi_to_this_person, dtype: object\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check each feature for outliers based on the 2 standard variation rule. \n",
    "for feat in df.columns:\n",
    "    try:\n",
    "        lim_h = np.mean(df[feat][df[feat] <> \"NaN\"]) + 2*np.std(df[feat][df[feat] <> \"NaN\"])\n",
    "        print df[(df[feat] > lim_h) & (df[feat] <> \"NaN\")][feat]\n",
    "        print\n",
    "    except TypeError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The previous code checked for each numerical feature which observation was lying outside of a 2 standard deviation range from the mean. This helped to recognize observations that were often lying outside of the most common values.\n",
    "\n",
    "This is how I saw that there is a \"Total\" observation include in the dataset that must aggregate part of the other observations. This line is not useful for the classifier, and may even lead it into error. Hence, it is dropped from the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bonus': 97343619,\n",
       " 'deferral_payments': 32083396,\n",
       " 'deferred_income': -27992891,\n",
       " 'director_fees': 1398517,\n",
       " 'email_address': 'NaN',\n",
       " 'exercised_stock_options': 311764000,\n",
       " 'expenses': 5235198,\n",
       " 'from_messages': 'NaN',\n",
       " 'from_poi_to_this_person': 'NaN',\n",
       " 'from_this_person_to_poi': 'NaN',\n",
       " 'loan_advances': 83925000,\n",
       " 'long_term_incentive': 48521928,\n",
       " 'other': 42667589,\n",
       " 'poi': False,\n",
       " 'restricted_stock': 130322299,\n",
       " 'restricted_stock_deferred': -7576788,\n",
       " 'salary': 26704229,\n",
       " 'shared_receipt_with_poi': 'NaN',\n",
       " 'to_messages': 'NaN',\n",
       " 'total_payments': 309886585,\n",
       " 'total_stock_value': 434509511}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove the TOTAL entry, as it appears clearcly as a general outlier. \n",
    "data_dict.pop(\"TOTAL\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working on the features\n",
    "\n",
    "### Adding new features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 3 new features that I was willing to implement. The “to_messages_poi_ratio” feature is the ratio between the number of emails from a person to a poi and the total number of emails sent by this person. It seems to me more relevant to work with this ratio, as I would expect PoI to send more often emails to other PoIs. \n",
    "\n",
    "In the same extent, I added the “from_poi_to_this_person” and the “shared_receipt_with_poi” features which calculate similar ratios with the emails received and shared by a person.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Task 3: Create new feature(s)\n",
    "# Adding the ratio of messages received, shared and sent to POIs\n",
    "for name in data_dict.keys():\n",
    "    try:\n",
    "        data_dict[name][\"to_messages_poi_ratio\"] = \\\n",
    "            1.0*data_dict[name][\"from_this_person_to_poi\"]/data_dict[name][\"from_messages\"]\n",
    "        data_dict[name][\"from_messages_poi_ratio\"] = \\\n",
    "            1.0*data_dict[name][\"from_poi_to_this_person\"]/data_dict[name][\"to_messages\"]\n",
    "        data_dict[name][\"shared_messages_poi_ratio\"] = \\\n",
    "            1.0*data_dict[name][\"shared_receipt_with_poi\"]/data_dict[name][\"to_messages\"]\n",
    "    except TypeError:\n",
    "        data_dict[name][\"to_messages_poi_ratio\"] = \"NaN\"\n",
    "        data_dict[name][\"from_messages_poi_ratio\"] = \"NaN\"\n",
    "        data_dict[name][\"shared_messages_poi_ratio\"] = \"NaN\"\n",
    "\n",
    "features_list.append(\"to_messages_poi_ratio\")\n",
    "features_list.append(\"from_messages_poi_ratio\")\n",
    "features_list.append(\"shared_messages_poi_ratio\")\n",
    "        \n",
    "# Store to my_dataset for easy export below.\n",
    "my_dataset = data_dict\n",
    "\n",
    "# Extract features and labels from dataset for local testing\n",
    "data = featureFormat(my_dataset, features_list, sort_keys=True)\n",
    "labels, features = targetFeatureSplit(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification performance with the initial feature list: \n",
      "['poi', 'salary', 'to_messages', 'deferral_payments', 'total_payments', 'exercised_stock_options', 'bonus', 'restricted_stock', 'shared_receipt_with_poi', 'restricted_stock_deferred', 'total_stock_value', 'expenses', 'loan_advances', 'from_messages', 'other', 'from_this_person_to_poi', 'director_fees', 'deferred_income', 'long_term_incentive', 'from_poi_to_this_person']\n",
      "RandomForestClassifier(bootstrap=True, class_weight='balanced',\n",
      "            criterion='gini', max_depth=None, max_features='auto',\n",
      "            max_leaf_nodes=None, min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
      "            oob_score=False, random_state=42, verbose=0, warm_start=False)\n",
      "\tAccuracy: 0.85680\tPrecision: 0.33772\tRecall: 0.07700\tF1: 0.12541\tF2: 0.09106\n",
      "\tTotal predictions: 15000\tTrue positives:  154\tFalse positives:  302\tFalse negatives: 1846\tTrue negatives: 12698\n",
      "\n",
      "Classification performance with the extended feature list: \n",
      "['poi', 'salary', 'to_messages', 'deferral_payments', 'total_payments', 'exercised_stock_options', 'bonus', 'restricted_stock', 'shared_receipt_with_poi', 'restricted_stock_deferred', 'total_stock_value', 'expenses', 'loan_advances', 'from_messages', 'other', 'from_this_person_to_poi', 'director_fees', 'deferred_income', 'long_term_incentive', 'from_poi_to_this_person', 'to_messages_poi_ratio', 'from_messages_poi_ratio', 'shared_messages_poi_ratio']\n",
      "RandomForestClassifier(bootstrap=True, class_weight='balanced',\n",
      "            criterion='gini', max_depth=None, max_features='auto',\n",
      "            max_leaf_nodes=None, min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
      "            oob_score=False, random_state=42, verbose=0, warm_start=False)\n",
      "\tAccuracy: 0.86540\tPrecision: 0.48148\tRecall: 0.12350\tF1: 0.19658\tF2: 0.14507\n",
      "\tTotal predictions: 15000\tTrue positives:  247\tFalse positives:  266\tFalse negatives: 1753\tTrue negatives: 12734\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# initial feature list to be compared with the extended one\n",
    "features_list_init = ['poi']\n",
    "for feat in df.columns:\n",
    "    if feat not in ['poi', 'email_address']:\n",
    "        features_list_init.append(feat)\n",
    "\n",
    "# Using a Random Forest Classifier to study the impact of adding these new features\n",
    "clf = RandomForestClassifier(class_weight='balanced', random_state=42)\n",
    "print \"Classification performance with the initial feature list: \"\n",
    "print features_list_init\n",
    "test_classifier(clf, my_dataset, features_list_init)\n",
    "\n",
    "print \"Classification performance with the extended feature list: \"\n",
    "print features_list\n",
    "test_classifier(clf, my_dataset, features_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apparently, adding these new features seems to provide more of usable information. Tuning the hyper parameters mayeven increase the usefulness of these added features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tree-based Feature Selection "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following chunk of code runs a random forest classifier to determine each feature importance ratio. I use a 100-folds stratified shuffle split in order to avoid overfitting issues by cross-validating my selection. \n",
    "\n",
    "If a feature importance ratio is lower than 1%, the feature is deleted from the features list. Then the algorithm restarts with the new features list, until there are only important features left."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['poi', 'salary', 'to_messages', 'deferral_payments', 'total_payments', 'exercised_stock_options', 'bonus', 'restricted_stock', 'shared_receipt_with_poi', 'restricted_stock_deferred', 'total_stock_value', 'expenses', 'loan_advances', 'from_messages', 'other', 'from_this_person_to_poi', 'director_fees', 'deferred_income', 'long_term_incentive', 'from_poi_to_this_person', 'to_messages_poi_ratio', 'from_messages_poi_ratio', 'shared_messages_poi_ratio']\n",
      "\n",
      "deleting features if their importance is lower than 0.01%\n",
      "deleting  deferral_payments   0.0096149068323\n",
      "deleting  total_stock_value   0.0\n",
      "deleting  other   0.0\n",
      "deleting  from_poi_to_this_person   0.0\n",
      "\n",
      "deleting features if their importance is lower than 0.01%\n",
      "deleting  restricted_stock_deferred   0.0099537037037\n",
      "deleting  from_messages   0.0\n",
      "deleting  long_term_incentive   0.0\n",
      "\n",
      "deleting features if their importance is lower than 0.01%\n",
      "deleting  loan_advances   0.0076171875\n",
      "deleting  deferred_income   0.0\n",
      "\n",
      "deleting features if their importance is lower than 0.01%\n",
      "deleting  director_fees   0.0\n",
      "\n",
      "deleting features if their importance is lower than 0.01%\n",
      "End\n",
      "\n",
      "Tree-based selected list of features\n",
      "['poi', 'salary', 'to_messages', 'total_payments', 'exercised_stock_options', 'bonus', 'restricted_stock', 'shared_receipt_with_poi', 'expenses', 'from_this_person_to_poi', 'to_messages_poi_ratio', 'from_messages_poi_ratio', 'shared_messages_poi_ratio']\n"
     ]
    }
   ],
   "source": [
    "# features list to be reduced\n",
    "features_list_selected = list(features_list)\n",
    "print features_list_selected\n",
    "\n",
    "# StratifiedShuffleSplit\n",
    "\n",
    "n_feat_del = 0\n",
    "stop = False\n",
    "while not(stop):\n",
    "    n_feat_del_ini = n_feat_del\n",
    "    # Extract features and labels from dataset for local testing\n",
    "    data = featureFormat(my_dataset, features_list_selected, sort_keys=True)\n",
    "    labels_sel, features_sel = targetFeatureSplit(data)\n",
    "    \n",
    "    # Using a 100-folds stratified shuffle split to to feature selection with cross-validation\n",
    "    folds = 100\n",
    "    cv = StratifiedShuffleSplit(labels_sel, folds, random_state = 42)\n",
    "    \n",
    "    feat_imp = [0 for i in range(len(features_sel))]\n",
    "    for train_idx, test_idx in cv:\n",
    "        features_train = [features_sel[i] for i in train_idx]\n",
    "        features_test  = [features_sel[i] for i in test_idx]\n",
    "        labels_train   = [labels_sel[i] for i in train_idx]\n",
    "        labels_test    = [labels_sel[i] for i in test_idx]\n",
    "\n",
    "        # Choose the most important features with a random forest classifier\n",
    "        clf = RandomForestClassifier(random_state=42)\n",
    "        clf.fit(features_train, labels_train)\n",
    "        feat_imp = [feat_imp[i] + clf.feature_importances_[i]/folds for i in range(len(clf.feature_importances_))]\n",
    "        \n",
    "    print\n",
    "    print \"deleting features if their importance is lower than 0.01%\"\n",
    "    for i in range(len(feat_imp)):\n",
    "        if feat_imp[i] < 0.01:\n",
    "            n_feat_del += 1\n",
    "            print \"deleting \", features_list_selected[i+1], \" \", clf.feature_importances_[i]\n",
    "            del features_list_selected[i+1]\n",
    "    if n_feat_del_ini == n_feat_del:\n",
    "        print \"End\"\n",
    "        stop = True\n",
    "\n",
    "print\n",
    "print \"Tree-based selected list of features\"\n",
    "print features_list_selected\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The performance brought by this selected features list is then compared to the one brought by the entire feature list using a Random Forest Classifier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing:  RandomForestClassifier(bootstrap=True, class_weight='balanced',\n",
      "            criterion='gini', max_depth=None, max_features='auto',\n",
      "            max_leaf_nodes=None, min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
      "            oob_score=False, random_state=42, verbose=0, warm_start=False)\n",
      "Random Forest trained with the entire feature list: \n",
      "RandomForestClassifier(bootstrap=True, class_weight='balanced',\n",
      "            criterion='gini', max_depth=None, max_features='auto',\n",
      "            max_leaf_nodes=None, min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
      "            oob_score=False, random_state=42, verbose=0, warm_start=False)\n",
      "\tAccuracy: 0.86540\tPrecision: 0.48148\tRecall: 0.12350\tF1: 0.19658\tF2: 0.14507\n",
      "\tTotal predictions: 15000\tTrue positives:  247\tFalse positives:  266\tFalse negatives: 1753\tTrue negatives: 12734\n",
      "\n",
      "Random Forest trained with the Tree-based selected feature list: \n",
      "RandomForestClassifier(bootstrap=True, class_weight='balanced',\n",
      "            criterion='gini', max_depth=None, max_features='auto',\n",
      "            max_leaf_nodes=None, min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
      "            oob_score=False, random_state=42, verbose=0, warm_start=False)\n",
      "\tAccuracy: 0.86300\tPrecision: 0.45299\tRecall: 0.13250\tF1: 0.20503\tF2: 0.15434\n",
      "\tTotal predictions: 15000\tTrue positives:  265\tFalse positives:  320\tFalse negatives: 1735\tTrue negatives: 12680\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = RandomForestClassifier(class_weight='balanced', random_state=42)\n",
    "print \"testing: \", clf\n",
    "\n",
    "# test the classifier with the testing function provided\n",
    "print \"Random Forest trained with the entire feature list: \"\n",
    "test_classifier(clf, my_dataset, features_list)\n",
    "\n",
    "print \"Random Forest trained with the Tree-based selected feature list: \"\n",
    "test_classifier(clf, my_dataset, features_list_selected)\n",
    "print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this feature selection seems to only slightly increase the performance for the standard random forest classifier.\n",
    "\n",
    "For the rest of my work, **I will continue with the entire features list**, but I will also implement a Principal Component Analysis to reduce the dimensionnality of my features and tackle the overfitting risk. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Trying some basic estimators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standard SVC, Random Forest and Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true negatives:  13000\n",
      "false negatives:  2000\n",
      "false positives:  0\n",
      "true positives:  0\n",
      "Got a divide by zero when trying out: Pipeline(steps=[('scaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('svc', SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape=None, degree=3, gamma='auto', kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False))])\n",
      "Precision or recall may be undefined due to a lack of true positive predicitons.\n",
      "\n",
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False)\n",
      "\tAccuracy: 0.86140\tPrecision: 0.43857\tRecall: 0.14100\tF1: 0.21339\tF2: 0.16314\n",
      "\tTotal predictions: 15000\tTrue positives:  282\tFalse positives:  361\tFalse negatives: 1718\tTrue negatives: 12639\n",
      "\n",
      "\n",
      "Pipeline(steps=[('scaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('logreg', LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False))])\n",
      "\tAccuracy: 0.84067\tPrecision: 0.31393\tRecall: 0.16450\tF1: 0.21588\tF2: 0.18181\n",
      "\tTotal predictions: 15000\tTrue positives:  329\tFalse positives:  719\tFalse negatives: 1671\tTrue negatives: 12281\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Creating a pipeline to include scaling when using SVC\n",
    "estimators_svc = [\n",
    "    ('scaler',StandardScaler()),\n",
    "    ('svc', SVC())]\n",
    "pipe_svc = Pipeline(estimators_svc)\n",
    "\n",
    "# Creating a pipeline to include scaling when using Logistic regression\n",
    "estimators_logreg = [\n",
    "    ('scaler',StandardScaler()),\n",
    "    ('logreg', LogisticRegression())]\n",
    "pipe_logreg = Pipeline(estimators_logreg)\n",
    "\n",
    "\n",
    "for clf in [pipe_svc, RandomForestClassifier(), pipe_logreg]:\n",
    "    # test the classifier with the testing function provided\n",
    "    # a 1000-folds cross-validation is done and different metrics are calculated (accuracy, precision, recall, custom f-scores)\n",
    "    # to assess the classifier performance.\n",
    "    test_classifier(clf, my_dataset, features_list)\n",
    "    print\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the standard classification algorithms, without scaling, dimensionality reduction ,or hyper parameter tuning does not provide a sufficient level of performance.\n",
    "\n",
    "The Random Forest classifier gives the best results when looking at the f1 and f2 scores. But as it is an ensemble classifier, it may be better suited to tackle overfitting per default. \n",
    "\n",
    "The SVC classifier is not well performing as it classifies all the users as non-PoI. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning\n",
    "\n",
    "Machine Learning algorithms are very dependent from hyper parameters. These paramaters can for instance play a strong role when dealing with overfitting. The goal of this chapter is to optimize the main parameters of the previous algorithms in order to increase the classification performance. In order to determine the best parameters for this Enron Dataset, a Grid Search Analysis is done on different Classifiers. \n",
    "\n",
    "The Grid Search approach trains a classifier for each hyper parameter combination of values and conducts a 10-folds cross-validation on the provided data set to select the best estimator. An f-score similar to the one calculated by the tester.py file is used to compare the different classifiers. I used then the test_classifier function from the tester.py file to calculate the classifier performance.\n",
    "\n",
    "In addition to optimizing the Classification algorithm itself, I also work on pipeline estimators that also include a standard scaler and a PCA transformation to increase the classification algorithm performance.\n",
    "\n",
    "### Grid Search Optimization: Scaling and SVC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting the classifier to the training set\n",
      "done in 44.050s\n",
      "Best estimator found by grid search:\n",
      "Pipeline(steps=[('scaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('svc', SVC(C=0.10000000000000001, cache_size=200, class_weight='balanced', coef0=0.0,\n",
      "  decision_function_shape=None, degree=2, gamma=1.0, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False))])\n",
      "Testing the best estimator\n",
      "Pipeline(steps=[('scaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('svc', SVC(C=0.10000000000000001, cache_size=200, class_weight='balanced', coef0=0.0,\n",
      "  decision_function_shape=None, degree=2, gamma=1.0, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False))])\n",
      "\tAccuracy: 0.27440\tPrecision: 0.15523\tRecall: 1.00000\tF1: 0.26874\tF2: 0.47884\n",
      "\tTotal predictions: 15000\tTrue positives: 2000\tFalse positives: 10884\tFalse negatives:    0\tTrue negatives: 2116\n",
      "\n",
      "done in 1.609s\n"
     ]
    }
   ],
   "source": [
    "# Creating a pipeline to include scaling when using SVC\n",
    "estimators = [\n",
    "    ('scaler',StandardScaler()),\n",
    "    ('svc', SVC(class_weight=\"balanced\", degree=2))]\n",
    "pipe = Pipeline(estimators)\n",
    "\n",
    "# Creating a stratified shuffle split to replace the standard Grid Search cross-validation\n",
    "cv = StratifiedShuffleSplit(labels, 100, random_state = 42)\n",
    "\n",
    "print \"Fitting the classifier to the training set\"\n",
    "t0 = time()\n",
    "\n",
    "# the linear and polynomial kernels are not included here. After some first tests, the test_classifier function \n",
    "# or the grid search algorythm gets stuck. Without PCA, the classification may not be done properly with linear kernels.\n",
    "# The polynomial kernel was not working either with the PCA\n",
    "param_grid = {\n",
    "    'svc__kernel':['sigmoid', 'rbf'], \n",
    "    'svc__C': np.logspace(-3, 3, 7),\n",
    "    'svc__gamma':np.logspace(-3, 3, 7)\n",
    "          }\n",
    "\n",
    "clf = GridSearchCV(pipe, param_grid, scoring=scorer_f, cv=cv)\n",
    "clf = clf.fit(features, labels)\n",
    "print \"done in %0.3fs\" % (time() - t0)\n",
    "print \"Best estimator found by grid search:\"\n",
    "print clf.best_estimator_\n",
    "\n",
    "print \"Testing the best estimator\"\n",
    "t0 = time()\n",
    "test_classifier(clf.best_estimator_, my_dataset, features_list)\n",
    "print \"done in %0.3fs\" % (time() - t0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid Search Optimization: Scaling, PCA and SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting the classifier to the training set\n",
      "done in 1206.864s\n",
      "Best estimator found by grid search:\n",
      "Pipeline(steps=[('scaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('pca', RandomizedPCA(copy=True, iterated_power=3, n_components=10, random_state=None,\n",
      "       whiten=True)), ('svc', SVC(C=10.0, cache_size=200, class_weight='balanced', coef0=0.0,\n",
      "  decision_function_shape=None, degree=2, gamma=0.001, kernel='linear',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False))])\n",
      "Testing the best estimator\n",
      "Pipeline(steps=[('scaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('pca', RandomizedPCA(copy=True, iterated_power=3, n_components=10, random_state=None,\n",
      "       whiten=True)), ('svc', SVC(C=10.0, cache_size=200, class_weight='balanced', coef0=0.0,\n",
      "  decision_function_shape=None, degree=2, gamma=0.001, kernel='linear',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False))])\n",
      "\tAccuracy: 0.69413\tPrecision: 0.21967\tRecall: 0.50700\tF1: 0.30653\tF2: 0.40187\n",
      "\tTotal predictions: 15000\tTrue positives: 1014\tFalse positives: 3602\tFalse negatives:  986\tTrue negatives: 9398\n",
      "\n",
      "done in 5.036s\n"
     ]
    }
   ],
   "source": [
    "estimators = [\n",
    "    ('scaler',StandardScaler()),\n",
    "    ('pca', RandomizedPCA(whiten=True)), \n",
    "    ('svc', SVC(class_weight=\"balanced\", degree=2))]\n",
    "pipe = Pipeline(estimators)\n",
    "\n",
    "# Creating a stratified shuffle split to replace the standard Grid Search cross-validation\n",
    "cv = StratifiedShuffleSplit(labels, 100, random_state = 42)\n",
    "\n",
    "param_grid = {\n",
    "    'pca__n_components' : [4, 6, 10, 15, 20],\n",
    "    'svc__kernel':['linear', 'sigmoid'], \n",
    "    'svc__C': np.logspace(-3, 3, 7),\n",
    "    'svc__gamma':np.logspace(-3, 3, 7)\n",
    "             }\n",
    "\n",
    "#    'svc__kernel':['linear', 'sigmoid', 'rbf', 'poly'], \n",
    "clf = GridSearchCV(pipe, param_grid, scoring=scorer_f, cv=cv)\n",
    "\n",
    "print \"Fitting the classifier to the training set\"\n",
    "t0 = time()\n",
    "clf.fit(features, labels)\n",
    "print \"done in %0.3fs\" % (time() - t0)\n",
    "print \"Best estimator found by grid search:\"\n",
    "print clf.best_estimator_\n",
    "\n",
    "print \"Testing the best estimator\"\n",
    "t0 = time()\n",
    "test_classifier(clf.best_estimator_, my_dataset, features_list)\n",
    "print \"done in %0.3fs\" % (time() - t0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid Search Optimization: Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting the classifier to the training set\n",
      "done in 606.900s\n",
      "Best estimator found by grid search:\n",
      "RandomForestClassifier(bootstrap=True, class_weight='balanced',\n",
      "            criterion='gini', max_depth=None, max_features='auto',\n",
      "            max_leaf_nodes=None, min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=15, n_jobs=-1,\n",
      "            oob_score=False, random_state=42, verbose=0, warm_start=False)\n",
      "Testing the best estimator\n",
      "RandomForestClassifier(bootstrap=True, class_weight='balanced',\n",
      "            criterion='gini', max_depth=None, max_features='auto',\n",
      "            max_leaf_nodes=None, min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=15, n_jobs=-1,\n",
      "            oob_score=False, random_state=42, verbose=0, warm_start=False)\n",
      "\tAccuracy: 0.86673\tPrecision: 0.50082\tRecall: 0.15250\tF1: 0.23381\tF2: 0.17714\n",
      "\tTotal predictions: 15000\tTrue positives:  305\tFalse positives:  304\tFalse negatives: 1695\tTrue negatives: 12696\n",
      "\n",
      "done in 303.589s\n"
     ]
    }
   ],
   "source": [
    "# Creating a stratified shuffle split to replace the standard Grid Search cross-validation\n",
    "cv = StratifiedShuffleSplit(labels, 100, random_state = 42)\n",
    "\n",
    "print \"Fitting the classifier to the training set\"\n",
    "t0 = time()\n",
    "param_grid = {\n",
    "         'n_estimators': [1, 5, 10, 15, 20],\n",
    "         'min_samples_leaf' : [1, 2, 3] \n",
    "          }\n",
    "\n",
    "clf = GridSearchCV(RandomForestClassifier(class_weight='balanced', random_state=42, n_jobs=-1), \n",
    "                   param_grid, scoring=scorer_f, cv=cv)\n",
    "clf = clf.fit(features, labels)\n",
    "print \"done in %0.3fs\" % (time() - t0)\n",
    "print \"Best estimator found by grid search:\"\n",
    "print clf.best_estimator_\n",
    "\n",
    "print \"Testing the best estimator\"\n",
    "t0 = time()\n",
    "test_classifier(clf.best_estimator_, my_dataset, features_list)\n",
    "print \"done in %0.3fs\" % (time() - t0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid Search Optimization: PCA & Random Forest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting the classifier to the training set\n",
      "done in 2873.080s\n",
      "Best estimator found by grid search:\n",
      "Pipeline(steps=[('scaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('pca', RandomizedPCA(copy=True, iterated_power=3, n_components=10, random_state=None,\n",
      "       whiten=True)), ('randomforest', RandomForestClassifier(bootstrap=True, class_weight='balanced',\n",
      "            criterion='gini', max...timators=20, n_jobs=-1,\n",
      "            oob_score=False, random_state=42, verbose=0, warm_start=False))])\n",
      "Testing the best estimator\n",
      "Pipeline(steps=[('scaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('pca', RandomizedPCA(copy=True, iterated_power=3, n_components=10, random_state=None,\n",
      "       whiten=True)), ('randomforest', RandomForestClassifier(bootstrap=True, class_weight='balanced',\n",
      "            criterion='gini', max...timators=20, n_jobs=-1,\n",
      "            oob_score=False, random_state=42, verbose=0, warm_start=False))])\n",
      "\tAccuracy: 0.83733\tPrecision: 0.23236\tRecall: 0.09550\tF1: 0.13536\tF2: 0.10825\n",
      "\tTotal predictions: 15000\tTrue positives:  191\tFalse positives:  631\tFalse negatives: 1809\tTrue negatives: 12369\n",
      "\n",
      "done in 318.682s\n"
     ]
    }
   ],
   "source": [
    "estimators = [\n",
    "    ('scaler',StandardScaler()),\n",
    "     ('pca', RandomizedPCA(whiten=True)), \n",
    "    ('randomforest', RandomForestClassifier(class_weight='balanced', random_state=42, n_jobs=-1))]\n",
    "pipe = Pipeline(estimators)\n",
    "\n",
    "# Creating a stratified shuffle split to replace the standard Grid Search cross-validation\n",
    "cv = StratifiedShuffleSplit(labels, 100, random_state = 42)\n",
    "\n",
    "param_grid = {\n",
    "    'pca__n_components':[4, 6, 10, 15, 20],\n",
    "    'randomforest__n_estimators':[1, 5, 10, 15, 20],\n",
    "    'randomforest__min_samples_leaf':[1, 2, 3] \n",
    "             }\n",
    "clf = GridSearchCV(pipe, param_grid, scoring=scorer_f, cv=cv)\n",
    "\n",
    "print \"Fitting the classifier to the training set\"\n",
    "t0 = time()\n",
    "clf = clf.fit(features, labels)\n",
    "print \"done in %0.3fs\" % (time() - t0)\n",
    "print \"Best estimator found by grid search:\"\n",
    "print clf.best_estimator_\n",
    "\n",
    "print \"Testing the best estimator\"\n",
    "t0 = time()\n",
    "test_classifier(clf.best_estimator_, my_dataset, features_list)\n",
    "print \"done in %0.3fs\" % (time() - t0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid Search Optimization: Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting the classifier to the training set\n",
      "done in 90.614s\n",
      "Best estimator found by grid search:\n",
      "Pipeline(steps=[('scaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('logreg', LogisticRegression(C=10.0, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='ovr', n_jobs=1, penalty='l1', random_state=42,\n",
      "          solver='liblinear', tol=0.0001, verbose=0, warm_start=False))])\n",
      "Testing the best estimator\n",
      "Pipeline(steps=[('scaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('logreg', LogisticRegression(C=10.0, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='ovr', n_jobs=1, penalty='l1', random_state=42,\n",
      "          solver='liblinear', tol=0.0001, verbose=0, warm_start=False))])\n",
      "\tAccuracy: 0.79853\tPrecision: 0.33301\tRecall: 0.50950\tF1: 0.40277\tF2: 0.46067\n",
      "\tTotal predictions: 15000\tTrue positives: 1019\tFalse positives: 2041\tFalse negatives:  981\tTrue negatives: 10959\n",
      "\n",
      "done in 6.401s\n"
     ]
    }
   ],
   "source": [
    "# Creating a pipeline to include scaling when using Logistic regression\n",
    "estimators = [('scaler',StandardScaler()),\n",
    "              ('logreg', LogisticRegression(class_weight=\"balanced\", random_state=42))]\n",
    "pipe = Pipeline(estimators)\n",
    "\n",
    "# Creating a stratified shuffle split to replace the standard Grid Search cross-validation\n",
    "cv = StratifiedShuffleSplit(labels, 100, random_state = 42)\n",
    "\n",
    "print \"Fitting the classifier to the training set\"\n",
    "t0 = time()\n",
    "param_grid = {\n",
    "         'logreg__C': np.logspace(-7, 7, 15),\n",
    "         'logreg__penalty' : [\"l1\", \"l2\"] \n",
    "          }\n",
    "\n",
    "clf = GridSearchCV(pipe, param_grid, scoring=scorer_f, cv=cv)\n",
    "clf = clf.fit(features, labels)\n",
    "print \"done in %0.3fs\" % (time() - t0)\n",
    "print \"Best estimator found by grid search:\"\n",
    "print clf.best_estimator_\n",
    "\n",
    "print \"Testing the best estimator\"\n",
    "t0 = time()\n",
    "test_classifier(clf.best_estimator_, my_dataset, features_list)\n",
    "print \"done in %0.3fs\" % (time() - t0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Grid Search Optimization: Scaling, PCA and Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting the classifier to the training set\n",
      "done in 186.484s\n",
      "Best estimator found by grid search:\n",
      "Pipeline(steps=[('scaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('pca', RandomizedPCA(copy=True, iterated_power=3, n_components=15, random_state=None,\n",
      "       whiten=True)), ('logreg', LogisticRegression(C=1.0, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='ovr', n_jobs=-1, penalty='l2', random_state=42,\n",
      "          solver='liblinear', tol=0.0001, verbose=0, warm_start=False))])\n",
      "Testing the best estimator\n",
      "Pipeline(steps=[('scaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('pca', RandomizedPCA(copy=True, iterated_power=3, n_components=15, random_state=None,\n",
      "       whiten=True)), ('logreg', LogisticRegression(C=1.0, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='ovr', n_jobs=-1, penalty='l2', random_state=42,\n",
      "          solver='liblinear', tol=0.0001, verbose=0, warm_start=False))])\n",
      "\tAccuracy: 0.74700\tPrecision: 0.25578\tRecall: 0.47000\tF1: 0.33128\tF2: 0.40257\n",
      "\tTotal predictions: 15000\tTrue positives:  940\tFalse positives: 2735\tFalse negatives: 1060\tTrue negatives: 10265\n",
      "\n",
      "done in 2.238s\n"
     ]
    }
   ],
   "source": [
    "estimators = [('scaler',StandardScaler()),\n",
    "              ('pca', RandomizedPCA(whiten=True)), \n",
    "              ('logreg', LogisticRegression(class_weight=\"balanced\", random_state=42, n_jobs=-1))]\n",
    "pipe = Pipeline(estimators)\n",
    "\n",
    "# Creating a stratified shuffle split to replace the standard Grid Search cross-validation\n",
    "cv = StratifiedShuffleSplit(labels, 100, random_state = 42)\n",
    "\n",
    "param_grid = {\n",
    "    'pca__n_components' : [4, 6, 10, 15, 20],\n",
    "    'logreg__C': np.logspace(-7, 7, 15),\n",
    "    'logreg__penalty' : [\"l1\", \"l2\"] \n",
    "             }\n",
    "clf = GridSearchCV(pipe, param_grid, scoring=scorer_f, cv=cv)\n",
    "\n",
    "print \"Fitting the classifier to the training set\"\n",
    "t0 = time()\n",
    "clf.fit(features, labels)\n",
    "print \"done in %0.3fs\" % (time() - t0)\n",
    "print \"Best estimator found by grid search:\"\n",
    "print clf.best_estimator_\n",
    "\n",
    "print \"Testing the best estimator\"\n",
    "t0 = time()\n",
    "test_classifier(clf.best_estimator_, my_dataset, features_list)\n",
    "print \"done in %0.3fs\" % (time() - t0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "The test_classifier function conducts a 1000-folds cross validation with the given classifier and the given feature list. It prints out different performance metrics (accuracy, precision, recall, f-scores) that help us assess the Classifier performance. The precision and the recall are the 2 main metrics that I was trying to maximize. The goal was to achieve  precision and recal values higher than 0.3.\n",
    "\n",
    "In this specific case:\n",
    "- Recall is the ratio of correctly identified PoI to total number of real PoI. \n",
    "- Precision is the ratio of correctly identified PoI to total number of identified PoI.\n",
    "\n",
    "In general, scaling the features and reducing their dimensionality should help to increase the classifier performance. \n",
    "\n",
    "But to my surprise, the best classification performance is provided by the simple optimized Logistic Regression. Moreover, this algorithm is really faster to train than the SVC and the Random Forest.\n",
    "\n",
    "I suspect that, as the C hyper parameter from the logistic regression algorithm has been developed to tackle the issue of overfitting, it may mitigate the need to use a dimensionality reduction approach. It also confirms that logistic regression does not need scaling to be properly trained. \n",
    "\n",
    "As a consequence, I decided to implement this optimized algorithm in the poi_id file. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bibliography\n",
    "\n",
    "\n",
    "- Sickit Learn examples : http://scikit-learn.org/ (RandomForestClassifier, AdaBoostClassifier, LogisticRegression, Pipeline, GridSearchCV, StratifiedShuffleSplit, RandomizedPCA, SVC)\n",
    "- Python Machine Learning, Sebastian Raschka, 2015, Packt Publishing\n",
    "- Udacity Nanoprogram - Machine Learning Course"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
